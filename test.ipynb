{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14fc8e72",
   "metadata": {},
   "source": [
    "# 测试ProstT5 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe40f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import re\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('/t9k/mnt/AMP/weights/ProstT5-Distilled-12l/final_model', do_lower_case=False)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/t9k/mnt/AMP/weights/ProstT5-Distilled-12l/final_model\").to(device)\n",
    "\n",
    "# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "model.float() if device.type=='cpu' else model.half()\n",
    "print(model)\n",
    "\n",
    "# prepare your protein sequences/structures as a list.\n",
    "# Amino acid sequences are expected to be upper-case (\"PRTEINO\" below)\n",
    "# while 3Di-sequences need to be lower-case.\n",
    "sequence_examples = [\"PRTEINO\", \"SEQWENCE\"]\n",
    "min_len = min([ len(s) for s in sequence_examples])\n",
    "max_len = max([ len(s) for s in sequence_examples])\n",
    "\n",
    "# replace all rare/ambiguous amino acids by X (3Di sequences does not have those) and introduce white-space between all sequences (AAs and 3Di)\n",
    "sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "\n",
    "# add pre-fixes accordingly. For the translation from AAs to 3Di, you need to prepend \"<AA2fold>\"\n",
    "sequence_examples = [ \"<AA2fold>\" + \" \" + s for s in sequence_examples]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids = tokenizer.batch_encode_plus(sequence_examples,\n",
    "                                  add_special_tokens=True,\n",
    "                                  padding=\"longest\",\n",
    "                                  return_tensors='pt').to(device)\n",
    "\n",
    "# Generation configuration for \"folding\" (AA-->3Di)\n",
    "gen_kwargs_aa2fold = {\n",
    "                  \"do_sample\": True,\n",
    "                  \"num_beams\": 3, \n",
    "                  \"top_p\" : 0.95, \n",
    "                  \"temperature\" : 1.2, \n",
    "                  \"top_k\" : 6,\n",
    "                  \"repetition_penalty\" : 1.2,\n",
    "}\n",
    "\n",
    "# translate from AA to 3Di (AA-->3Di)\n",
    "with torch.no_grad():\n",
    "  translations = model.generate( \n",
    "              ids.input_ids, \n",
    "              attention_mask=ids.attention_mask, \n",
    "              max_length=max_len, # max length of generated text\n",
    "              min_length=min_len, # minimum length of the generated text\n",
    "              early_stopping=True, # stop early if end-of-text token is generated\n",
    "              num_return_sequences=1, # return only a single sequence\n",
    "              **gen_kwargs_aa2fold\n",
    "  )\n",
    "# Decode and remove white-spaces between tokens\n",
    "decoded_translations = tokenizer.batch_decode( translations, skip_special_tokens=True )\n",
    "structure_sequences = [ \"\".join(ts.split(\" \")) for ts in decoded_translations ] # predicted 3Di strings\n",
    "\n",
    "\n",
    "\n",
    "print(\"Input AA sequences: \", sequence_examples)\n",
    "print(\"Predicted 3Di sequences: \", structure_sequences)\n",
    "# Now we can use the same model and invert the translation logic\n",
    "# to generate an amino acid sequence from the predicted 3Di-sequence (3Di-->AA)\n",
    "\n",
    "# add pre-fixes accordingly. For the translation from 3Di to AA (3Di-->AA), you need to prepend \"<fold2AA>\"\n",
    "sequence_examples_backtranslation = [ \"<fold2AA>\" + \" \" + s for s in decoded_translations]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids_backtranslation = tokenizer.batch_encode_plus(sequence_examples_backtranslation,\n",
    "                                  add_special_tokens=True,\n",
    "                                  padding=\"longest\",\n",
    "                                  return_tensors='pt').to(device)\n",
    "\n",
    "# Example generation configuration for \"inverse folding\" (3Di-->AA)\n",
    "gen_kwargs_fold2AA = {\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\" : 0.85,\n",
    "            \"temperature\" : 1.0,\n",
    "            \"top_k\" : 3,\n",
    "            \"repetition_penalty\" : 1.2,\n",
    "}\n",
    "\n",
    "# translate from 3Di to AA (3Di-->AA)\n",
    "with torch.no_grad():\n",
    "  backtranslations = model.generate( \n",
    "              ids_backtranslation.input_ids, \n",
    "              attention_mask=ids_backtranslation.attention_mask, \n",
    "              max_length=max_len, # max length of generated text\n",
    "              min_length=min_len, # minimum length of the generated text\n",
    "              #early_stopping=True, # stop early if end-of-text token is generated; only needed for beam-search\n",
    "              num_return_sequences=1, # return only a single sequence\n",
    "              **gen_kwargs_fold2AA\n",
    ")\n",
    "# Decode and remove white-spaces between tokens\n",
    "decoded_backtranslations = tokenizer.batch_decode( backtranslations, skip_special_tokens=True )\n",
    "aminoAcid_sequences = [ \"\".join(ts.split(\" \")) for ts in decoded_backtranslations ] # predicted amino acid strings\n",
    "\n",
    "print(\"input 3Di sequences: \", sequence_examples_backtranslation)\n",
    "print(\"Predicted back-translated AA sequences: \", aminoAcid_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890272ad",
   "metadata": {},
   "source": [
    "# 测试T2struc load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "from transformers import AutoTokenizer, EsmTokenizer\n",
    "from collections import OrderedDict\n",
    "from transformers import GenerationConfig\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "from rich.logging import RichHandler\n",
    "import math\n",
    "from models import StructureTokenPredictionModel\n",
    "import logging \n",
    "\n",
    "logger = logging.getLogger(\"rich\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def load_T2Struc_tokenizers(cfg):\n",
    "    text_tokenizer =  AutoTokenizer.from_pretrained(cfg.model[\"lm\"])\n",
    "    structure_tokenizer = EsmTokenizer.from_pretrained(cfg.model[\"tokenizer\"])\n",
    "    return text_tokenizer, structure_tokenizer\n",
    "\n",
    "\n",
    "def load_T2Struc(model_dir_or_weight: str,\n",
    "                dtype: torch.dtype = torch.bfloat16,\n",
    "                device: torch.device = DEVICE):\n",
    "    \"\"\"\n",
    "    Load T2Struc model from a directory (containing config.yaml + pytorch_model.bin)\n",
    "    or directly from a weight file path.\n",
    "    \"\"\"\n",
    "    # 1) Resolve paths\n",
    "    if os.path.isdir(model_dir_or_weight):\n",
    "        model_dir = model_dir_or_weight\n",
    "        cfg_path = os.path.join(model_dir, \"config.yaml\")\n",
    "        weight_path = os.path.join(model_dir, \"pytorch_model.bin\")\n",
    "    else:\n",
    "        weight_path = model_dir_or_weight\n",
    "        model_dir = os.path.dirname(weight_path)\n",
    "        cfg_path = os.path.join(model_dir, \"config.yaml\")\n",
    "\n",
    "    if not os.path.isfile(cfg_path):\n",
    "        raise FileNotFoundError(f\"config.yaml not found: {cfg_path}\")\n",
    "    if not os.path.isfile(weight_path):\n",
    "        raise FileNotFoundError(f\"checkpoint not found: {weight_path}\")\n",
    "\n",
    "    # 2) Load config\n",
    "    cfg = OmegaConf.load(cfg_path)\n",
    "\n",
    "    # 3) Build model (on CPU first, in desired dtype)\n",
    "    model = StructureTokenPredictionModel(cfg.model).to(dtype=dtype)\n",
    "\n",
    "    # 4) Load weights (to CPU)\n",
    "    logger.info(f\"Loading T2Struc weights from: {weight_path}\")\n",
    "    state_dict = torch.load(weight_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    # 5) Move to device + eval\n",
    "    model = model.to(device=device).eval()\n",
    "\n",
    "    text_tokenizer, structure_tokenizer = load_T2Struc_tokenizers(cfg)\n",
    "\n",
    "\n",
    "    return model, text_tokenizer, structure_tokenizer\n",
    "\n",
    "T2Struc, text_tokenizer, structure_tokenizer = load_T2Struc(\"/t9k/mnt/AMP/weights/T2struc-fined/2025-11-27_21-54-10/final_model\")\n",
    "print(T2Struc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849f106",
   "metadata": {},
   "source": [
    "# 测试合并模型 end to end \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4592500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import logging\n",
    "from omegaconf import OmegaConf\n",
    "# 假设 EndToEndModel 保存在 models 文件夹下\n",
    "from models.EndToEndModel import EndToEndModel\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_and_convert_weights():\n",
    "    # ===================== Paths (Based on your notebook) ===================== #\n",
    "    t2struc_path = \"/t9k/mnt/AMP/weights/T2struc-fined/2025-11-27_21-54-10/final_model\"\n",
    "    prostt5_path = \"/t9k/mnt/AMP/weights/ProstT5-Distilled-12l/final_model\"\n",
    "    output_path = \"/t9k/mnt/AMP/weights/EndToEndModel_merged\"\n",
    "    \n",
    "    device = torch.device('cpu') # Load on CPU to save memory during merge\n",
    "\n",
    "    logger.info(\"Loading T2Struc Config...\")\n",
    "    t2struc_cfg_path = os.path.join(t2struc_path, \"config.yaml\")\n",
    "    t2struc_cfg = OmegaConf.load(t2struc_cfg_path)\n",
    "    \n",
    "    # ===================== Initialize Empty EndToEndModel ===================== #\n",
    "    logger.info(\"Initializing EndToEndModel structure...\")\n",
    "    # 我们传入 t2struc 的 model 配置部分，和 ProstT5 的路径\n",
    "    model = EndToEndModel(t2struc_cfg.model, prostt5_path)\n",
    "    \n",
    "    # ===================== Load T2Struc Weights ===================== #\n",
    "    t2struc_weight_path = os.path.join(t2struc_path, \"pytorch_model.bin\")\n",
    "    logger.info(f\"Loading T2Struc weights from {t2struc_weight_path}...\")\n",
    "    t2struc_state_dict = torch.load(t2struc_weight_path, map_location=\"cpu\")\n",
    "    \n",
    "    # T2Struc 的权重在 EndToEndModel 中对应的 prefix 是 \"t2struc.\"\n",
    "    # 原始权重里的 key 类似 \"lm.shared.weight\", \"plm.transformer...\"\n",
    "    # 我们需要给它们加上前缀\n",
    "    new_t2struc_state_dict = {}\n",
    "    for k, v in t2struc_state_dict.items():\n",
    "        new_key = \"t2struc.\" + k\n",
    "        new_t2struc_state_dict[new_key] = v\n",
    "        \n",
    "    # 加载到模型中 (strict=False, 因为还有 prostt5 和 projector 的权重没加载)\n",
    "    keys = model.load_state_dict(new_t2struc_state_dict, strict=False)\n",
    "    logger.info(f\"T2Struc weights loaded. Missing keys (expected ProstT5/Proj): {len(keys.missing_keys)}\")\n",
    "\n",
    "    # ===================== Load ProstT5 Weights ===================== #\n",
    "    # ProstT5 通常是一个标准的 HuggingFace 模型，可能是 pytorch_model.bin 或 model.safetensors\n",
    "    # 这里的路径是文件夹，我们让 T5ForConditionalGeneration 自己加载，然后我们提取它的 state_dict\n",
    "    logger.info(f\"Loading ProstT5 weights from {prostt5_path}...\")\n",
    "    \n",
    "    # 这里的技巧是：EndToEndModel 初始化时已经通过 from_pretrained 加载了 ProstT5 的初始权重（如果路径正确）\n",
    "    # 在 EndToEndModel.__init__ 中：self.prostt5 = T5ForConditionalGeneration(config)\n",
    "    # 如果那里用的是 Config 初始化而不是 from_pretrained(path)，权重是随机的。\n",
    "    # 为了保险起见，我们在这里显式加载一次权重并赋值。\n",
    "    \n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    temp_prostt5 = T5ForConditionalGeneration.from_pretrained(prostt5_path)\n",
    "    prostt5_state_dict = temp_prostt5.state_dict()\n",
    "    \n",
    "    # ProstT5 的权重在 EndToEndModel 中对应的 prefix 是 \"prostt5.\"\n",
    "    new_prostt5_state_dict = {}\n",
    "    for k, v in prostt5_state_dict.items():\n",
    "        new_key = \"prostt5.\" + k\n",
    "        new_prostt5_state_dict[new_key] = v\n",
    "    \n",
    "    # 加载 ProstT5 权重\n",
    "    keys = model.load_state_dict(new_prostt5_state_dict, strict=False)\n",
    "    logger.info(f\"ProstT5 weights loaded. Missing keys (expected Proj only): {len(keys.missing_keys)}\")\n",
    "    \n",
    "    # 检查 Projector 权重\n",
    "    # Projector 是随机初始化的，不需要加载，但应该在 missing_keys 中\n",
    "    projector_keys = [k for k in keys.missing_keys if \"projector\" in k]\n",
    "    logger.info(f\"Projector keys (randomly initialized): {projector_keys}\")\n",
    "\n",
    "    # ===================== Save Merged Model ===================== #\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    save_path = os.path.join(output_path, \"pytorch_model.bin\")\n",
    "    logger.info(f\"Saving combined model to {save_path}...\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    # 同时保存 Config 以便后续加载\n",
    "    # 保存 T2Struc 的 config\n",
    "    OmegaConf.save(t2struc_cfg, os.path.join(output_path, \"t2struc_config.yaml\"))\n",
    "    # 保存 ProstT5 的 config\n",
    "    model.prostt5.config.save_pretrained(output_path)\n",
    "    \n",
    "    logger.info(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_and_convert_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62229a87",
   "metadata": {},
   "source": [
    "## 测试end to end model流程能否跑通\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "088dc3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/t9k/mnt/.conda/envs/text2amp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /t9k/mnt/AMP/weights/EndToEndModel_merged/final_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/t9k/mnt/.conda/envs/text2amp/lib/python3.8/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from /t9k/mnt/AMP/weights/EndToEndModel_merged/final_model/pytorch_model_bfloat16.bin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizers...\n",
      "Loading structure tokenizer from checkpoints/structure_tokenizer...\n",
      "Loaded ProstT5 tokenizer from merged dir: /t9k/mnt/AMP/weights/EndToEndModel_merged/final_model\n",
      "Model and tokenizers loaded successfully!\n",
      "Structure Tokenizer - start_id: 1, stop_id: 2, pad_id: 0\n",
      "Total model parameters: 2668102912\n",
      "Input: generate an  peptide with alpha-helical structure.\n",
      "Generated Sequence: M T G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "from transformers import AutoTokenizer, EsmTokenizer, T5Tokenizer\n",
    "from models.EndToEndModel import EndToEndModel  # 确保能导入你定义的类\n",
    "\n",
    "\n",
    "def load_end_to_end_model(merged_model_dir, device='cuda'):\n",
    "    \"\"\"\n",
    "    加载合并后的 EndToEndModel 和所需的 Tokenizers\n",
    "\n",
    "    Args:\n",
    "        merged_model_dir: 上一步保存权重的目录 (例如 /t9k/mnt/AMP/weights/EndToEndModel_merged)\n",
    "        device: 'cuda' or 'cpu' or torch.device\n",
    "    \"\"\"\n",
    "    print(f\"Loading model from {merged_model_dir}...\")\n",
    "\n",
    "    # ================= 1. 加载配置 =================\n",
    "    t2struc_config_path = os.path.join(merged_model_dir, \"t2struc_config.yaml\")\n",
    "    if not os.path.exists(t2struc_config_path):\n",
    "        raise FileNotFoundError(f\"Config not found at {t2struc_config_path}\")\n",
    "\n",
    "    t2struc_cfg = OmegaConf.load(t2struc_config_path)\n",
    "\n",
    "    # ================= 2. 实例化模型 =================\n",
    "    model = EndToEndModel(t2struc_cfg.model, prostt5_config_path=merged_model_dir)\n",
    "\n",
    "    # ================= 3. 加载合并后的权重 =================\n",
    "    weight_path = os.path.join(merged_model_dir, \"pytorch_model_bfloat16.bin\")\n",
    "    print(f\"Loading weights from {weight_path}...\")\n",
    "\n",
    "    state_dict = torch.load(weight_path, map_location='cpu')\n",
    "\n",
    "    # strict=True 确保所有键都匹配（T2Struc, ProstT5, Projectors）\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    model.to(device,dtype=torch.bfloat16)\n",
    "    model.eval()\n",
    "\n",
    "    # ================= 4. 加载 Tokenizers =================\n",
    "    print(\"Loading tokenizers...\")\n",
    "\n",
    "    # (1) Text Tokenizer\n",
    "    text_tokenizer_path = t2struc_cfg.model.lm\n",
    "    text_tokenizer = AutoTokenizer.from_pretrained(text_tokenizer_path)\n",
    "\n",
    "    # (2) Structure Tokenizer\n",
    "    structure_tokenizer_path = t2struc_cfg.model.tokenizer\n",
    "    print(f\"Loading structure tokenizer from {structure_tokenizer_path}...\")\n",
    "    structure_tokenizer = EsmTokenizer.from_pretrained(structure_tokenizer_path)\n",
    "\n",
    "    # (3) ProstT5 Tokenizer\n",
    "    try:\n",
    "        prostt5_tokenizer = T5Tokenizer.from_pretrained(merged_model_dir, do_lower_case=False)\n",
    "        print(f\"Loaded ProstT5 tokenizer from merged dir: {merged_model_dir}\")\n",
    "    except Exception:\n",
    "        prostt5_orig_path = '/t9k/mnt/AMP/weights/ProstT5-Distilled-12l/final_model'\n",
    "        print(f\"Tokenizer not found in merged dir, loading from {prostt5_orig_path}\")\n",
    "        prostt5_tokenizer = T5Tokenizer.from_pretrained(prostt5_orig_path, do_lower_case=False)\n",
    "\n",
    "    print(\"Model and tokenizers loaded successfully!\")\n",
    "\n",
    "    return model, text_tokenizer, structure_tokenizer, prostt5_tokenizer\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1. 基础设置\n",
    "# -----------------------------\n",
    "merged_path = \"/t9k/mnt/AMP/weights/EndToEndModel_merged/final_model\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------------\n",
    "# 2. 加载模型\n",
    "# -----------------------------\n",
    "model, text_tokenizer, structure_tokenizer, prostt5_tokenizer = load_end_to_end_model(merged_path, device)\n",
    "model.eval()\n",
    "\n",
    "# 输出 structure tokenizer 的 special token ids\n",
    "start_id = structure_tokenizer.cls_token_id\n",
    "stop_id = structure_tokenizer.eos_token_id\n",
    "pad_id = structure_tokenizer.pad_token_id\n",
    "print(f\"Structure Tokenizer - start_id: {start_id}, stop_id: {stop_id}, pad_id: {pad_id}\")\n",
    "\n",
    "# 输出 model 的参数总量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total model parameters: {total_params}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. 构造测试输入\n",
    "# -----------------------------\n",
    "input_text = \"generate an  peptide with alpha-helical structure.\"\n",
    "\n",
    "# 编码文本\n",
    "text_inputs = text_tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "\n",
    "# 构造 batch（Generate 模式只需要 text）\n",
    "batch = {\n",
    "    \"text_ids\": text_inputs[\"input_ids\"],\n",
    "    \"text_masks\": text_inputs[\"attention_mask\"],\n",
    "    \"max_structure_len\": 64,  # 中间结构生成最大长度\n",
    "    \"max_seq_len\": 50         # 最终序列生成最大长度\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 4. 推理（Generate）\n",
    "# -----------------------------\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(batch)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. 解码结果\n",
    "# -----------------------------\n",
    "generated_seq = prostt5_tokenizer.batch_decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. 打印结果\n",
    "# -----------------------------\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated Sequence: {generated_seq[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae1cfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndToEndModel(\n",
      "  (t2struc): StructureTokenPredictionModel(\n",
      "    (lm): T5EncoderModel(\n",
      "      (shared): Embedding(32128, 1024)\n",
      "      (encoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32128, 1024)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 16)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-23): 23 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                  (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (plm): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(25, 1280)\n",
      "        (wpe): Embedding(1024, 1280)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-35): 36 x GPT2Block(\n",
      "            (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2Attention(\n",
      "              (c_attn): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (crossattention): GPT2Attention(\n",
      "              (c_attn): Conv1D()\n",
      "              (q_attn): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_cross_attn): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D()\n",
      "              (c_proj): Conv1D()\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1280, out_features=25, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (prostt5): T5ForConditionalGeneration(\n",
      "    (shared): Embedding(150, 1024)\n",
      "    (encoder): T5Stack(\n",
      "      (embed_tokens): Embedding(150, 1024)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 32)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "                (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1-11): 11 x T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "                (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (decoder): T5Stack(\n",
      "      (embed_tokens): Embedding(150, 1024)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 32)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "                (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1-11): 11 x T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "                (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
      "                (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=1024, out_features=150, bias=False)\n",
      "  )\n",
      "  (structure_projector): Linear(in_features=1280, out_features=1024, bias=True)\n",
      "  (text_projector): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e571730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import T5Config, T5ForConditionalGeneration\n",
    "# from .StructureTokenPredictionModel import StructureTokenPredictionModel\n",
    "# from .abs_model import ABSmodule\n",
    "\n",
    "\n",
    "# class EndToEndModel(ABSmodule):\n",
    "#     \"\"\"\n",
    "#     New logic (your requirement):\n",
    "#     - Structure token ids (discrete) -> explicit structure string\n",
    "#     - Encode structure string with ProstT5.encoder to obtain structure_hidden_states (d_model=1024)\n",
    "#     - Keep text_hidden_states from t2struc.infer_text(batch)\n",
    "#     - Project (if needed) and concat: [text_embeds_proj ; structure_embeds]\n",
    "#     - Feed to ProstT5 via inputs_embeds + attention_mask\n",
    "\n",
    "#     Notes:\n",
    "#     - This requires:\n",
    "#         1) id2struct: mapping from 0..(structure_vocab-1) -> string token (usually single char)\n",
    "#         2) struct_tokenizer: tokenizer that can convert structure strings into ProstT5-compatible input_ids\n",
    "#            (i.e., ids must be within ProstT5 vocab size)\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         t2struc_config,\n",
    "#         prostt5_config_path,\n",
    "#         struct_tokenizer=None,\n",
    "#         id2struct=None,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # ===================== 1. Structure Prediction Module (T2Struc) ===================== #\n",
    "#         self.t2struc = StructureTokenPredictionModel(t2struc_config)\n",
    "\n",
    "#         self.structure_hidden_size = self.t2struc.plm.config.hidden_size  # 1280 (GPT2 hidden)\n",
    "#         self.text_hidden_size = self.t2struc.lm.config.hidden_size        # 1024 (T5 encoder hidden)\n",
    "\n",
    "#         # ===================== 2. Sequence Generation Module (ProstT5) ===================== #\n",
    "#         try:\n",
    "#             prostt5_config = T5Config.from_pretrained(prostt5_config_path, local_files_only=True)\n",
    "#         except Exception:\n",
    "#             print(f\"Warning: Could not load config from {prostt5_config_path}, using default T5Config.\")\n",
    "#             prostt5_config = T5Config()\n",
    "\n",
    "#         self.prostt5 = T5ForConditionalGeneration(prostt5_config)\n",
    "#         self.prostt5_hidden_size = self.prostt5.config.d_model  # 1024\n",
    "\n",
    "#         # ===================== 3. Projection Layers ===================== #\n",
    "#         # NOTE: Under your new logic, structure_hidden_states come from ProstT5.encoder, already 1024,\n",
    "#         # so structure_projector is no longer needed for the structure path.\n",
    "#         # We keep it ONLY in case you later want to experiment; by default it is not used.\n",
    "#         self.structure_projector = nn.Linear(self.structure_hidden_size, self.prostt5_hidden_size)\n",
    "#         nn.init.xavier_uniform_(self.structure_projector.weight)\n",
    "#         if self.structure_projector.bias is not None:\n",
    "#             nn.init.constant_(self.structure_projector.bias, 0)\n",
    "\n",
    "#         # Text Projector: 1024 -> 1024\n",
    "#         if self.text_hidden_size != self.prostt5_hidden_size:\n",
    "#             self.text_projector = nn.Linear(self.text_hidden_size, self.prostt5_hidden_size)\n",
    "#         else:\n",
    "#             self.text_projector = nn.Identity()\n",
    "\n",
    "#         # ===================== 4. Structure string conversion + tokenizer ===================== #\n",
    "#         self.struct_tokenizer = struct_tokenizer  # MUST be provided for new logic\n",
    "#         self.id2struct = id2struct                # MUST be provided for new logic\n",
    "\n",
    "#     def infer_text(self, batch, **kwargs):\n",
    "#         return self.t2struc.infer_text(batch)\n",
    "\n",
    "#     # ------------------------- helper: ids -> structure strings ------------------------- #\n",
    "#     def ids_to_struct_strings(\n",
    "#         self,\n",
    "#         structure_ids: torch.LongTensor,\n",
    "#         pad_token_id: int,\n",
    "#         eos_token_id: int,\n",
    "#         bos_token_id: int,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         structure_ids: [B, L]\n",
    "#         Return: List[str] length B\n",
    "#         \"\"\"\n",
    "#         if self.id2struct is None:\n",
    "#             raise ValueError(\n",
    "#                 \"id2struct is required but not provided. \"\n",
    "#                 \"Please pass id2struct (list or dict) mapping structure token id -> string token.\"\n",
    "#             )\n",
    "\n",
    "#         # Accept list or dict\n",
    "#         def id_to_token(i: int) -> str:\n",
    "#             if isinstance(self.id2struct, dict):\n",
    "#                 return self.id2struct[i]\n",
    "#             return self.id2struct[i]  # list/tuple\n",
    "\n",
    "#         B, L = structure_ids.shape\n",
    "#         out = []\n",
    "#         for b in range(B):\n",
    "#             toks = []\n",
    "#             for t in structure_ids[b].tolist():\n",
    "#                 if t == pad_token_id:\n",
    "#                     continue\n",
    "#                 if t == bos_token_id:\n",
    "#                     continue\n",
    "#                 if t == eos_token_id:\n",
    "#                     break\n",
    "#                 toks.append(id_to_token(t))\n",
    "#             out.append(\"\".join(toks))\n",
    "#         return out\n",
    "\n",
    "#     # ------------------------- helper: encode structure via ProstT5.encoder ------------------------- #\n",
    "#     def encode_structure_with_prostt5_encoder(\n",
    "#         self,\n",
    "#         struct_strings,\n",
    "#         device,\n",
    "#         max_length: int = 512,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         struct_strings: List[str]\n",
    "#         returns:\n",
    "#           structure_hidden_states: [B, Ls2, 1024]\n",
    "#           structure_masks: [B, Ls2]\n",
    "#         \"\"\"\n",
    "#         if self.struct_tokenizer is None:\n",
    "#             raise ValueError(\n",
    "#                 \"struct_tokenizer is required but not provided. \"\n",
    "#                 \"It must convert structure strings into ProstT5-compatible input_ids.\"\n",
    "#             )\n",
    "\n",
    "#         struct_batch = self.struct_tokenizer(\n",
    "#             struct_strings,\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=max_length,\n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "#         structure_input_ids = struct_batch[\"input_ids\"].to(device)\n",
    "#         structure_masks = struct_batch[\"attention_mask\"].to(device)\n",
    "\n",
    "#         enc_out = self.prostt5.encoder(\n",
    "#             input_ids=structure_input_ids,\n",
    "#             attention_mask=structure_masks,\n",
    "#             return_dict=True\n",
    "#         )\n",
    "#         structure_hidden_states = enc_out.last_hidden_state\n",
    "#         return structure_hidden_states, structure_masks\n",
    "\n",
    "#     # ------------------------- forward (training) ------------------------- #\n",
    "#     def forward(self, batch):\n",
    "#         \"\"\"\n",
    "#         Training Forward Pass (new logic):\n",
    "#         - text_hidden_states from t2struc.infer_text\n",
    "#         - structure token ids from batch (teacher forcing or provided)\n",
    "#         - ids -> structure strings\n",
    "#         - ProstT5.encoder encodes structure strings to structure_hidden_states (1024)\n",
    "#         - concat text_embeds_proj + structure_hidden_states\n",
    "#         - ProstT5 forward with inputs_embeds + attention_mask + labels\n",
    "#         \"\"\"\n",
    "#         # 1) Text Encoding\n",
    "#         text_hidden_states, text_masks = self.t2struc.infer_text(batch)  # [B, Lt, 1024], [B, Lt]\n",
    "#         device = text_hidden_states.device\n",
    "\n",
    "#         # 2) Get special token ids for structure vocab\n",
    "#         plm_config = self.t2struc.plm.config\n",
    "#         vocab_size = self.t2struc.plm.transformer.wte.weight.shape[0]  # usually 25\n",
    "\n",
    "#         bos_token_id = plm_config.bos_token_id\n",
    "#         if bos_token_id is None or bos_token_id >= vocab_size:\n",
    "#             bos_token_id = 1\n",
    "#         eos_token_id = plm_config.eos_token_id\n",
    "#         if eos_token_id is None or eos_token_id >= vocab_size:\n",
    "#             eos_token_id = 2\n",
    "#         pad_token_id = plm_config.pad_token_id\n",
    "#         if pad_token_id is None or pad_token_id >= vocab_size:\n",
    "#             pad_token_id = 0\n",
    "\n",
    "#         # 3) Structure token ids (training): prefer explicit strings if user provides them\n",
    "#         #    - If batch[\"structure_strings\"] exists, use it directly.\n",
    "#         #    - Else use batch[\"structure_token_ids\"] to build strings.\n",
    "#         if \"structure_strings\" in batch and batch[\"structure_strings\"] is not None:\n",
    "#             struct_strings = batch[\"structure_strings\"]\n",
    "#         else:\n",
    "#             if \"structure_token_ids\" not in batch:\n",
    "#                 raise KeyError(\n",
    "#                     \"Training forward requires either batch['structure_strings'] or batch['structure_token_ids'].\"\n",
    "#                 )\n",
    "#             structure_token_ids = batch[\"structure_token_ids\"]\n",
    "#             if not torch.is_tensor(structure_token_ids):\n",
    "#                 structure_token_ids = torch.tensor(structure_token_ids, dtype=torch.long, device=device)\n",
    "#             else:\n",
    "#                 structure_token_ids = structure_token_ids.to(device)\n",
    "\n",
    "#             struct_strings = self.ids_to_struct_strings(\n",
    "#                 structure_token_ids,\n",
    "#                 pad_token_id=pad_token_id,\n",
    "#                 eos_token_id=eos_token_id,\n",
    "#                 bos_token_id=bos_token_id,\n",
    "#             )\n",
    "\n",
    "#         # 4) Encode structure strings via ProstT5.encoder\n",
    "#         structure_hidden_states, structure_masks = self.encode_structure_with_prostt5_encoder(\n",
    "#             struct_strings,\n",
    "#             device=device,\n",
    "#             max_length=batch.get(\"max_structure_len\", 512),\n",
    "#         )  # [B, Ls2, 1024], [B, Ls2]\n",
    "\n",
    "#         # 5) Project text (if needed) and concat (keep your original concat logic)\n",
    "#         text_embeds_proj = self.text_projector(text_hidden_states)  # [B, Lt, 1024]\n",
    "#         inputs_embeds = torch.cat([text_embeds_proj, structure_hidden_states], dim=1)  # [B, Lt+Ls2, 1024]\n",
    "#         attention_mask = torch.cat([text_masks.to(device), structure_masks], dim=1)    # [B, Lt+Ls2]\n",
    "\n",
    "#         # 6) ProstT5 forward\n",
    "#         prostt5_outputs = self.prostt5(\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             attention_mask=attention_mask,\n",
    "#             labels=batch.get(\"labels\", None),\n",
    "#             return_dict=True\n",
    "#         )\n",
    "\n",
    "#         return {\n",
    "#             \"loss\": prostt5_outputs.loss,\n",
    "#             \"logits\": prostt5_outputs.logits\n",
    "#         }\n",
    "\n",
    "#     # ------------------------- generate (inference) ------------------------- #\n",
    "#     @torch.no_grad()\n",
    "#     def generate(self, batch, generation_config=None):\n",
    "#         \"\"\"\n",
    "#         Inference Logic (new logic):\n",
    "#         - Text encoding via t2struc.infer_text\n",
    "#         - Structure discrete generation via t2struc.plm.generate\n",
    "#         - ids -> structure strings\n",
    "#         - ProstT5.encoder encodes structure strings -> structure_hidden_states\n",
    "#         - concat with text_embeds_proj and generate sequence via ProstT5.generate\n",
    "#         \"\"\"\n",
    "#         # 1) Text Encoding\n",
    "#         text_hidden_states, text_masks = self.t2struc.infer_text(batch)\n",
    "#         device = text_hidden_states.device\n",
    "\n",
    "#         # 2) Correct special IDs for your structure tokenizer settings\n",
    "#         plm_config = self.t2struc.plm.config\n",
    "#         vocab_size = self.t2struc.plm.transformer.wte.weight.shape[0]  # 25\n",
    "\n",
    "#         bos_token_id = plm_config.bos_token_id\n",
    "#         if bos_token_id is None or bos_token_id >= vocab_size:\n",
    "#             bos_token_id = 1\n",
    "#         eos_token_id = plm_config.eos_token_id\n",
    "#         if eos_token_id is None or eos_token_id >= vocab_size:\n",
    "#             eos_token_id = 2\n",
    "#         pad_token_id = plm_config.pad_token_id\n",
    "#         if pad_token_id is None or pad_token_id >= vocab_size:\n",
    "#             pad_token_id = 0\n",
    "\n",
    "#         # 3) Structure Generation (discrete)\n",
    "#         structure_gen_kwargs = {}\n",
    "#         seq_gen_kwargs = {}\n",
    "#         if generation_config:\n",
    "#             structure_gen_kwargs = generation_config.get(\"structure_gen_kwargs\", {}) or {}\n",
    "#             seq_gen_kwargs = generation_config.get(\"seq_gen_kwargs\", {}) or {}\n",
    "\n",
    "#         generated_structure = self.t2struc.plm.generate(\n",
    "#             input_ids=None,\n",
    "#             encoder_hidden_states=text_hidden_states,\n",
    "#             encoder_attention_mask=text_masks,\n",
    "#             bos_token_id=bos_token_id,\n",
    "#             eos_token_id=eos_token_id,\n",
    "#             pad_token_id=pad_token_id,\n",
    "#             max_length=batch.get(\"max_structure_len\", 512),\n",
    "#             return_dict_in_generate=True,\n",
    "#             **structure_gen_kwargs\n",
    "#         )\n",
    "#         generated_structure_ids = generated_structure.sequences  # [B, Ls]\n",
    "\n",
    "#         # 4) IDs -> explicit structure strings\n",
    "#         struct_strings = self.ids_to_struct_strings(\n",
    "#             generated_structure_ids,\n",
    "#             pad_token_id=pad_token_id,\n",
    "#             eos_token_id=eos_token_id,\n",
    "#             bos_token_id=bos_token_id,\n",
    "#         )\n",
    "\n",
    "#         # 5) ProstT5.encoder encodes structure strings -> structure_hidden_states\n",
    "#         structure_hidden_states, structure_masks = self.encode_structure_with_prostt5_encoder(\n",
    "#             struct_strings,\n",
    "#             device=device,\n",
    "#             max_length=batch.get(\"max_structure_len\", 512),\n",
    "#         )  # [B, Ls2, 1024], [B, Ls2]\n",
    "\n",
    "#         # 6) Keep your concat logic with text\n",
    "#         text_embeds_proj = self.text_projector(text_hidden_states)\n",
    "#         inputs_embeds = torch.cat([text_embeds_proj, structure_hidden_states], dim=1)\n",
    "#         attention_mask = torch.cat([text_masks.to(device), structure_masks], dim=1)\n",
    "\n",
    "#         # 7) Sequence generation (ProstT5)\n",
    "#         generated_sequence = self.prostt5.generate(\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             attention_mask=attention_mask,\n",
    "#             max_length=batch.get(\"max_seq_len\", 512),\n",
    "#             **seq_gen_kwargs\n",
    "#         )\n",
    "\n",
    "#         return {\n",
    "#             \"generated_structure_ids\": generated_structure_ids,\n",
    "#             \"generated_structure_strings\": struct_strings,\n",
    "#             \"generated_sequence_ids\": generated_sequence\n",
    "#         }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2amp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
