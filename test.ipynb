{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14fc8e72",
   "metadata": {},
   "source": [
    "# 测试ProstT5 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe40f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import re\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('/t9k/mnt/AMP/weights/ProstT5-Distilled-12l/final_model', do_lower_case=False)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/t9k/mnt/AMP/weights/ProstT5-Distilled-12l/final_model\").to(device)\n",
    "\n",
    "# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "model.float() if device.type=='cpu' else model.half()\n",
    "\n",
    "# prepare your protein sequences/structures as a list.\n",
    "# Amino acid sequences are expected to be upper-case (\"PRTEINO\" below)\n",
    "# while 3Di-sequences need to be lower-case.\n",
    "sequence_examples = [\"PRTEINO\", \"SEQWENCE\"]\n",
    "min_len = min([ len(s) for s in sequence_examples])\n",
    "max_len = max([ len(s) for s in sequence_examples])\n",
    "\n",
    "# replace all rare/ambiguous amino acids by X (3Di sequences does not have those) and introduce white-space between all sequences (AAs and 3Di)\n",
    "sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "\n",
    "# add pre-fixes accordingly. For the translation from AAs to 3Di, you need to prepend \"<AA2fold>\"\n",
    "sequence_examples = [ \"<AA2fold>\" + \" \" + s for s in sequence_examples]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids = tokenizer.batch_encode_plus(sequence_examples,\n",
    "                                  add_special_tokens=True,\n",
    "                                  padding=\"longest\",\n",
    "                                  return_tensors='pt').to(device)\n",
    "\n",
    "# Generation configuration for \"folding\" (AA-->3Di)\n",
    "gen_kwargs_aa2fold = {\n",
    "                  \"do_sample\": True,\n",
    "                  \"num_beams\": 3, \n",
    "                  \"top_p\" : 0.95, \n",
    "                  \"temperature\" : 1.2, \n",
    "                  \"top_k\" : 6,\n",
    "                  \"repetition_penalty\" : 1.2,\n",
    "}\n",
    "\n",
    "# translate from AA to 3Di (AA-->3Di)\n",
    "with torch.no_grad():\n",
    "  translations = model.generate( \n",
    "              ids.input_ids, \n",
    "              attention_mask=ids.attention_mask, \n",
    "              max_length=max_len, # max length of generated text\n",
    "              min_length=min_len, # minimum length of the generated text\n",
    "              early_stopping=True, # stop early if end-of-text token is generated\n",
    "              num_return_sequences=1, # return only a single sequence\n",
    "              **gen_kwargs_aa2fold\n",
    "  )\n",
    "# Decode and remove white-spaces between tokens\n",
    "decoded_translations = tokenizer.batch_decode( translations, skip_special_tokens=True )\n",
    "structure_sequences = [ \"\".join(ts.split(\" \")) for ts in decoded_translations ] # predicted 3Di strings\n",
    "\n",
    "\n",
    "\n",
    "print(\"Input AA sequences: \", sequence_examples)\n",
    "print(\"Predicted 3Di sequences: \", structure_sequences)\n",
    "# Now we can use the same model and invert the translation logic\n",
    "# to generate an amino acid sequence from the predicted 3Di-sequence (3Di-->AA)\n",
    "\n",
    "# add pre-fixes accordingly. For the translation from 3Di to AA (3Di-->AA), you need to prepend \"<fold2AA>\"\n",
    "sequence_examples_backtranslation = [ \"<fold2AA>\" + \" \" + s for s in decoded_translations]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids_backtranslation = tokenizer.batch_encode_plus(sequence_examples_backtranslation,\n",
    "                                  add_special_tokens=True,\n",
    "                                  padding=\"longest\",\n",
    "                                  return_tensors='pt').to(device)\n",
    "\n",
    "# Example generation configuration for \"inverse folding\" (3Di-->AA)\n",
    "gen_kwargs_fold2AA = {\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\" : 0.85,\n",
    "            \"temperature\" : 1.0,\n",
    "            \"top_k\" : 3,\n",
    "            \"repetition_penalty\" : 1.2,\n",
    "}\n",
    "\n",
    "# translate from 3Di to AA (3Di-->AA)\n",
    "with torch.no_grad():\n",
    "  backtranslations = model.generate( \n",
    "              ids_backtranslation.input_ids, \n",
    "              attention_mask=ids_backtranslation.attention_mask, \n",
    "              max_length=max_len, # max length of generated text\n",
    "              min_length=min_len, # minimum length of the generated text\n",
    "              #early_stopping=True, # stop early if end-of-text token is generated; only needed for beam-search\n",
    "              num_return_sequences=1, # return only a single sequence\n",
    "              **gen_kwargs_fold2AA\n",
    ")\n",
    "# Decode and remove white-spaces between tokens\n",
    "decoded_backtranslations = tokenizer.batch_decode( backtranslations, skip_special_tokens=True )\n",
    "aminoAcid_sequences = [ \"\".join(ts.split(\" \")) for ts in decoded_backtranslations ] # predicted amino acid strings\n",
    "\n",
    "print(\"input 3Di sequences: \", sequence_examples_backtranslation)\n",
    "print(\"Predicted back-translated AA sequences: \", aminoAcid_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890272ad",
   "metadata": {},
   "source": [
    "# 测试T2struc load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8482361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "from transformers import AutoTokenizer, EsmTokenizer\n",
    "from collections import OrderedDict\n",
    "from transformers import GenerationConfig\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "from rich.logging import RichHandler\n",
    "import math\n",
    "from models import StructureTokenPredictionModel\n",
    "import logging \n",
    "\n",
    "logger = logging.getLogger(\"rich\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def load_T2Struc_tokenizers(cfg):\n",
    "    text_tokenizer =  AutoTokenizer.from_pretrained(cfg.model[\"lm\"])\n",
    "    structure_tokenizer = EsmTokenizer.from_pretrained(cfg.model[\"tokenizer\"])\n",
    "    return text_tokenizer, structure_tokenizer\n",
    "\n",
    "\n",
    "def load_T2Struc(model_dir_or_weight: str,\n",
    "                dtype: torch.dtype = torch.bfloat16,\n",
    "                device: torch.device = DEVICE):\n",
    "    \"\"\"\n",
    "    Load T2Struc model from a directory (containing config.yaml + pytorch_model.bin)\n",
    "    or directly from a weight file path.\n",
    "    \"\"\"\n",
    "    # 1) Resolve paths\n",
    "    if os.path.isdir(model_dir_or_weight):\n",
    "        model_dir = model_dir_or_weight\n",
    "        cfg_path = os.path.join(model_dir, \"config.yaml\")\n",
    "        weight_path = os.path.join(model_dir, \"pytorch_model.bin\")\n",
    "    else:\n",
    "        weight_path = model_dir_or_weight\n",
    "        model_dir = os.path.dirname(weight_path)\n",
    "        cfg_path = os.path.join(model_dir, \"config.yaml\")\n",
    "\n",
    "    if not os.path.isfile(cfg_path):\n",
    "        raise FileNotFoundError(f\"config.yaml not found: {cfg_path}\")\n",
    "    if not os.path.isfile(weight_path):\n",
    "        raise FileNotFoundError(f\"checkpoint not found: {weight_path}\")\n",
    "\n",
    "    # 2) Load config\n",
    "    cfg = OmegaConf.load(cfg_path)\n",
    "\n",
    "    # 3) Build model (on CPU first, in desired dtype)\n",
    "    model = StructureTokenPredictionModel(cfg.model).to(dtype=dtype)\n",
    "\n",
    "    # 4) Load weights (to CPU)\n",
    "    logger.info(f\"Loading T2Struc weights from: {weight_path}\")\n",
    "    state_dict = torch.load(weight_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    # 5) Move to device + eval\n",
    "    model = model.to(device=device).eval()\n",
    "\n",
    "    text_tokenizer, structure_tokenizer = load_T2Struc_tokenizers(cfg)\n",
    "\n",
    "\n",
    "    return model, text_tokenizer, structure_tokenizer\n",
    "\n",
    "T2Struc, text_tokenizer, structure_tokenizer = load_T2Struc(\"/t9k/mnt/AMP/weights/T2struc-fined/2025-11-27_21-54-10/final_model\")\n",
    "# print(T2Struc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2amp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
