{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14fc8e72",
   "metadata": {},
   "source": [
    "# 测试ProstT5 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe40f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import re\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('/t9k/mnt/AMP/weights/ProstT5-Distilled-12l/final_model', do_lower_case=False)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/t9k/mnt/AMP/weights/ProstT5-Distilled-12l/final_model\").to(device)\n",
    "\n",
    "# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "model.float() if device.type=='cpu' else model.half()\n",
    "print(model)\n",
    "\n",
    "# prepare your protein sequences/structures as a list.\n",
    "# Amino acid sequences are expected to be upper-case (\"PRTEINO\" below)\n",
    "# while 3Di-sequences need to be lower-case.\n",
    "sequence_examples = [\"PRTEINO\", \"SEQWENCE\"]\n",
    "min_len = min([ len(s) for s in sequence_examples])\n",
    "max_len = max([ len(s) for s in sequence_examples])\n",
    "\n",
    "# replace all rare/ambiguous amino acids by X (3Di sequences does not have those) and introduce white-space between all sequences (AAs and 3Di)\n",
    "sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "\n",
    "# add pre-fixes accordingly. For the translation from AAs to 3Di, you need to prepend \"<AA2fold>\"\n",
    "sequence_examples = [ \"<AA2fold>\" + \" \" + s for s in sequence_examples]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids = tokenizer.batch_encode_plus(sequence_examples,\n",
    "                                  add_special_tokens=True,\n",
    "                                  padding=\"longest\",\n",
    "                                  return_tensors='pt').to(device)\n",
    "\n",
    "# Generation configuration for \"folding\" (AA-->3Di)\n",
    "gen_kwargs_aa2fold = {\n",
    "                  \"do_sample\": True,\n",
    "                  \"num_beams\": 3, \n",
    "                  \"top_p\" : 0.95, \n",
    "                  \"temperature\" : 1.2, \n",
    "                  \"top_k\" : 6,\n",
    "                  \"repetition_penalty\" : 1.2,\n",
    "}\n",
    "\n",
    "# translate from AA to 3Di (AA-->3Di)\n",
    "with torch.no_grad():\n",
    "  translations = model.generate( \n",
    "              ids.input_ids, \n",
    "              attention_mask=ids.attention_mask, \n",
    "              max_length=max_len, # max length of generated text\n",
    "              min_length=min_len, # minimum length of the generated text\n",
    "              early_stopping=True, # stop early if end-of-text token is generated\n",
    "              num_return_sequences=1, # return only a single sequence\n",
    "              **gen_kwargs_aa2fold\n",
    "  )\n",
    "# Decode and remove white-spaces between tokens\n",
    "decoded_translations = tokenizer.batch_decode( translations, skip_special_tokens=True )\n",
    "structure_sequences = [ \"\".join(ts.split(\" \")) for ts in decoded_translations ] # predicted 3Di strings\n",
    "\n",
    "\n",
    "\n",
    "print(\"Input AA sequences: \", sequence_examples)\n",
    "print(\"Predicted 3Di sequences: \", structure_sequences)\n",
    "# Now we can use the same model and invert the translation logic\n",
    "# to generate an amino acid sequence from the predicted 3Di-sequence (3Di-->AA)\n",
    "\n",
    "# add pre-fixes accordingly. For the translation from 3Di to AA (3Di-->AA), you need to prepend \"<fold2AA>\"\n",
    "sequence_examples_backtranslation = [ \"<fold2AA>\" + \" \" + s for s in decoded_translations]\n",
    "\n",
    "# tokenize sequences and pad up to the longest sequence in the batch\n",
    "ids_backtranslation = tokenizer.batch_encode_plus(sequence_examples_backtranslation,\n",
    "                                  add_special_tokens=True,\n",
    "                                  padding=\"longest\",\n",
    "                                  return_tensors='pt').to(device)\n",
    "\n",
    "# Example generation configuration for \"inverse folding\" (3Di-->AA)\n",
    "gen_kwargs_fold2AA = {\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\" : 0.85,\n",
    "            \"temperature\" : 1.0,\n",
    "            \"top_k\" : 3,\n",
    "            \"repetition_penalty\" : 1.2,\n",
    "}\n",
    "\n",
    "# translate from 3Di to AA (3Di-->AA)\n",
    "with torch.no_grad():\n",
    "  backtranslations = model.generate( \n",
    "              ids_backtranslation.input_ids, \n",
    "              attention_mask=ids_backtranslation.attention_mask, \n",
    "              max_length=max_len, # max length of generated text\n",
    "              min_length=min_len, # minimum length of the generated text\n",
    "              #early_stopping=True, # stop early if end-of-text token is generated; only needed for beam-search\n",
    "              num_return_sequences=1, # return only a single sequence\n",
    "              **gen_kwargs_fold2AA\n",
    ")\n",
    "# Decode and remove white-spaces between tokens\n",
    "decoded_backtranslations = tokenizer.batch_decode( backtranslations, skip_special_tokens=True )\n",
    "aminoAcid_sequences = [ \"\".join(ts.split(\" \")) for ts in decoded_backtranslations ] # predicted amino acid strings\n",
    "\n",
    "print(\"input 3Di sequences: \", sequence_examples_backtranslation)\n",
    "print(\"Predicted back-translated AA sequences: \", aminoAcid_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890272ad",
   "metadata": {},
   "source": [
    "# 测试T2struc load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "from transformers import AutoTokenizer, EsmTokenizer\n",
    "from collections import OrderedDict\n",
    "from transformers import GenerationConfig\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "from rich.logging import RichHandler\n",
    "import math\n",
    "from models import StructureTokenPredictionModel\n",
    "import logging \n",
    "\n",
    "logger = logging.getLogger(\"rich\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def load_T2Struc_tokenizers(cfg):\n",
    "    text_tokenizer =  AutoTokenizer.from_pretrained(cfg.model[\"lm\"])\n",
    "    structure_tokenizer = EsmTokenizer.from_pretrained(cfg.model[\"tokenizer\"])\n",
    "    return text_tokenizer, structure_tokenizer\n",
    "\n",
    "\n",
    "def load_T2Struc(model_dir_or_weight: str,\n",
    "                dtype: torch.dtype = torch.bfloat16,\n",
    "                device: torch.device = DEVICE):\n",
    "    \"\"\"\n",
    "    Load T2Struc model from a directory (containing config.yaml + pytorch_model.bin)\n",
    "    or directly from a weight file path.\n",
    "    \"\"\"\n",
    "    # 1) Resolve paths\n",
    "    if os.path.isdir(model_dir_or_weight):\n",
    "        model_dir = model_dir_or_weight\n",
    "        cfg_path = os.path.join(model_dir, \"config.yaml\")\n",
    "        weight_path = os.path.join(model_dir, \"pytorch_model.bin\")\n",
    "    else:\n",
    "        weight_path = model_dir_or_weight\n",
    "        model_dir = os.path.dirname(weight_path)\n",
    "        cfg_path = os.path.join(model_dir, \"config.yaml\")\n",
    "\n",
    "    if not os.path.isfile(cfg_path):\n",
    "        raise FileNotFoundError(f\"config.yaml not found: {cfg_path}\")\n",
    "    if not os.path.isfile(weight_path):\n",
    "        raise FileNotFoundError(f\"checkpoint not found: {weight_path}\")\n",
    "\n",
    "    # 2) Load config\n",
    "    cfg = OmegaConf.load(cfg_path)\n",
    "\n",
    "    # 3) Build model (on CPU first, in desired dtype)\n",
    "    model = StructureTokenPredictionModel(cfg.model).to(dtype=dtype)\n",
    "\n",
    "    # 4) Load weights (to CPU)\n",
    "    logger.info(f\"Loading T2Struc weights from: {weight_path}\")\n",
    "    state_dict = torch.load(weight_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    # 5) Move to device + eval\n",
    "    model = model.to(device=device).eval()\n",
    "\n",
    "    text_tokenizer, structure_tokenizer = load_T2Struc_tokenizers(cfg)\n",
    "\n",
    "\n",
    "    return model, text_tokenizer, structure_tokenizer\n",
    "\n",
    "T2Struc, text_tokenizer, structure_tokenizer = load_T2Struc(\"/t9k/mnt/AMP/weights/T2struc-fined/2025-11-27_21-54-10/final_model\")\n",
    "print(T2Struc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849f106",
   "metadata": {},
   "source": [
    "# 测试合并模型 end to end \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4592500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import logging\n",
    "from omegaconf import OmegaConf\n",
    "# 假设 EndToEndModel 保存在 models 文件夹下\n",
    "from models.EndToEndModel import EndToEndModel\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_and_convert_weights():\n",
    "    # ===================== Paths (Based on your notebook) ===================== #\n",
    "    t2struc_path = \"/t9k/mnt/AMP/weights/T2struc-fined/2025-11-27_21-54-10/final_model\"\n",
    "    prostt5_path = \"/t9k/mnt/AMP/weights/ProstT5-Distilled-12l/final_model\"\n",
    "    output_path = \"/t9k/mnt/AMP/weights/EndToEndModel_merged\"\n",
    "    \n",
    "    device = torch.device('cpu') # Load on CPU to save memory during merge\n",
    "\n",
    "    logger.info(\"Loading T2Struc Config...\")\n",
    "    t2struc_cfg_path = os.path.join(t2struc_path, \"config.yaml\")\n",
    "    t2struc_cfg = OmegaConf.load(t2struc_cfg_path)\n",
    "    \n",
    "    # ===================== Initialize Empty EndToEndModel ===================== #\n",
    "    logger.info(\"Initializing EndToEndModel structure...\")\n",
    "    # 我们传入 t2struc 的 model 配置部分，和 ProstT5 的路径\n",
    "    model = EndToEndModel(t2struc_cfg.model, prostt5_path)\n",
    "    \n",
    "    # ===================== Load T2Struc Weights ===================== #\n",
    "    t2struc_weight_path = os.path.join(t2struc_path, \"pytorch_model.bin\")\n",
    "    logger.info(f\"Loading T2Struc weights from {t2struc_weight_path}...\")\n",
    "    t2struc_state_dict = torch.load(t2struc_weight_path, map_location=\"cpu\")\n",
    "    \n",
    "    # T2Struc 的权重在 EndToEndModel 中对应的 prefix 是 \"t2struc.\"\n",
    "    # 原始权重里的 key 类似 \"lm.shared.weight\", \"plm.transformer...\"\n",
    "    # 我们需要给它们加上前缀\n",
    "    new_t2struc_state_dict = {}\n",
    "    for k, v in t2struc_state_dict.items():\n",
    "        new_key = \"t2struc.\" + k\n",
    "        new_t2struc_state_dict[new_key] = v\n",
    "        \n",
    "    # 加载到模型中 (strict=False, 因为还有 prostt5 和 projector 的权重没加载)\n",
    "    keys = model.load_state_dict(new_t2struc_state_dict, strict=False)\n",
    "    logger.info(f\"T2Struc weights loaded. Missing keys (expected ProstT5/Proj): {len(keys.missing_keys)}\")\n",
    "\n",
    "    # ===================== Load ProstT5 Weights ===================== #\n",
    "    # ProstT5 通常是一个标准的 HuggingFace 模型，可能是 pytorch_model.bin 或 model.safetensors\n",
    "    # 这里的路径是文件夹，我们让 T5ForConditionalGeneration 自己加载，然后我们提取它的 state_dict\n",
    "    logger.info(f\"Loading ProstT5 weights from {prostt5_path}...\")\n",
    "    \n",
    "    # 这里的技巧是：EndToEndModel 初始化时已经通过 from_pretrained 加载了 ProstT5 的初始权重（如果路径正确）\n",
    "    # 在 EndToEndModel.__init__ 中：self.prostt5 = T5ForConditionalGeneration(config)\n",
    "    # 如果那里用的是 Config 初始化而不是 from_pretrained(path)，权重是随机的。\n",
    "    # 为了保险起见，我们在这里显式加载一次权重并赋值。\n",
    "    \n",
    "    from transformers import T5ForConditionalGeneration\n",
    "    temp_prostt5 = T5ForConditionalGeneration.from_pretrained(prostt5_path)\n",
    "    prostt5_state_dict = temp_prostt5.state_dict()\n",
    "    \n",
    "    # ProstT5 的权重在 EndToEndModel 中对应的 prefix 是 \"prostt5.\"\n",
    "    new_prostt5_state_dict = {}\n",
    "    for k, v in prostt5_state_dict.items():\n",
    "        new_key = \"prostt5.\" + k\n",
    "        new_prostt5_state_dict[new_key] = v\n",
    "    \n",
    "    # 加载 ProstT5 权重\n",
    "    keys = model.load_state_dict(new_prostt5_state_dict, strict=False)\n",
    "    logger.info(f\"ProstT5 weights loaded. Missing keys (expected Proj only): {len(keys.missing_keys)}\")\n",
    "    \n",
    "    # 检查 Projector 权重\n",
    "    # Projector 是随机初始化的，不需要加载，但应该在 missing_keys 中\n",
    "    projector_keys = [k for k in keys.missing_keys if \"projector\" in k]\n",
    "    logger.info(f\"Projector keys (randomly initialized): {projector_keys}\")\n",
    "\n",
    "    # ===================== Save Merged Model ===================== #\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    save_path = os.path.join(output_path, \"pytorch_model.bin\")\n",
    "    logger.info(f\"Saving combined model to {save_path}...\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    # 同时保存 Config 以便后续加载\n",
    "    # 保存 T2Struc 的 config\n",
    "    OmegaConf.save(t2struc_cfg, os.path.join(output_path, \"t2struc_config.yaml\"))\n",
    "    # 保存 ProstT5 的 config\n",
    "    model.prostt5.config.save_pretrained(output_path)\n",
    "    \n",
    "    logger.info(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_and_convert_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62229a87",
   "metadata": {},
   "source": [
    "## 测试end to end model流程能否跑通\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "088dc3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/t9k/mnt/.conda/envs/text2amp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /t9k/mnt/AMP/weights/EndToEndModel_merged...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/t9k/mnt/.conda/envs/text2amp/lib/python3.8/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from /t9k/mnt/AMP/weights/EndToEndModel_merged/pytorch_model.bin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizers...\n",
      "Tokenizer not found in merged dir, loading from /t9k/mnt/AMP/weights/ProstT5-Distilled-12l/final_model\n",
      "Model and tokenizers loaded successfully!\n",
      "Structure Tokenizer - start_id: 1, stop_id: 2, pad_id: 0\n",
      "Total model parameters: 2668102912\n",
      "Input: generate an  peptide with alpha-helical structure.\n",
      "Generated Sequence: M T G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "from transformers import AutoTokenizer, EsmTokenizer, T5Tokenizer\n",
    "from models.EndToEndModel import EndToEndModel  # 确保能导入你定义的类\n",
    "\n",
    "\n",
    "def load_end_to_end_model(merged_model_dir, device='cuda'):\n",
    "    \"\"\"\n",
    "    加载合并后的 EndToEndModel 和所需的 Tokenizers\n",
    "\n",
    "    Args:\n",
    "        merged_model_dir: 上一步保存权重的目录 (例如 /t9k/mnt/AMP/weights/EndToEndModel_merged)\n",
    "        device: 'cuda' or 'cpu' or torch.device\n",
    "    \"\"\"\n",
    "    print(f\"Loading model from {merged_model_dir}...\")\n",
    "\n",
    "    # ================= 1. 加载配置 =================\n",
    "    t2struc_config_path = os.path.join(merged_model_dir, \"t2struc_config.yaml\")\n",
    "    if not os.path.exists(t2struc_config_path):\n",
    "        raise FileNotFoundError(f\"Config not found at {t2struc_config_path}\")\n",
    "\n",
    "    t2struc_cfg = OmegaConf.load(t2struc_config_path)\n",
    "\n",
    "    # ================= 2. 实例化模型 =================\n",
    "    model = EndToEndModel(t2struc_cfg.model, prostt5_config_path=merged_model_dir)\n",
    "\n",
    "    # ================= 3. 加载合并后的权重 =================\n",
    "    weight_path = os.path.join(merged_model_dir, \"pytorch_model.bin\")\n",
    "    print(f\"Loading weights from {weight_path}...\")\n",
    "\n",
    "    state_dict = torch.load(weight_path, map_location='cpu')\n",
    "\n",
    "    # strict=True 确保所有键都匹配（T2Struc, ProstT5, Projectors）\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # ================= 4. 加载 Tokenizers =================\n",
    "    print(\"Loading tokenizers...\")\n",
    "\n",
    "    # (1) Text Tokenizer\n",
    "    text_tokenizer_path = t2struc_cfg.model.lm\n",
    "    text_tokenizer = AutoTokenizer.from_pretrained(text_tokenizer_path)\n",
    "\n",
    "    # (2) Structure Tokenizer\n",
    "    structure_tokenizer_path = t2struc_cfg.model.tokenizer\n",
    "    structure_tokenizer = EsmTokenizer.from_pretrained(structure_tokenizer_path)\n",
    "\n",
    "    # (3) ProstT5 Tokenizer\n",
    "    try:\n",
    "        prostt5_tokenizer = T5Tokenizer.from_pretrained(merged_model_dir, do_lower_case=False)\n",
    "    except Exception:\n",
    "        prostt5_orig_path = '/t9k/mnt/AMP/weights/ProstT5-Distilled-12l/final_model'\n",
    "        print(f\"Tokenizer not found in merged dir, loading from {prostt5_orig_path}\")\n",
    "        prostt5_tokenizer = T5Tokenizer.from_pretrained(prostt5_orig_path, do_lower_case=False)\n",
    "\n",
    "    print(\"Model and tokenizers loaded successfully!\")\n",
    "\n",
    "    return model, text_tokenizer, structure_tokenizer, prostt5_tokenizer\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1. 基础设置\n",
    "# -----------------------------\n",
    "merged_path = \"/t9k/mnt/AMP/weights/EndToEndModel_merged\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------------\n",
    "# 2. 加载模型\n",
    "# -----------------------------\n",
    "model, text_tokenizer, structure_tokenizer, prostt5_tokenizer = load_end_to_end_model(merged_path, device)\n",
    "model.eval()\n",
    "\n",
    "# 输出 structure tokenizer 的 special token ids\n",
    "start_id = structure_tokenizer.cls_token_id\n",
    "stop_id = structure_tokenizer.eos_token_id\n",
    "pad_id = structure_tokenizer.pad_token_id\n",
    "print(f\"Structure Tokenizer - start_id: {start_id}, stop_id: {stop_id}, pad_id: {pad_id}\")\n",
    "\n",
    "# 输出 model 的参数总量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total model parameters: {total_params}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. 构造测试输入\n",
    "# -----------------------------\n",
    "input_text = \"generate an  peptide with alpha-helical structure.\"\n",
    "\n",
    "# 编码文本\n",
    "text_inputs = text_tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "text_inputs = {k: v.to(device) for k, v in text_inputs.items()}\n",
    "\n",
    "# 构造 batch（Generate 模式只需要 text）\n",
    "batch = {\n",
    "    \"text_ids\": text_inputs[\"input_ids\"],\n",
    "    \"text_masks\": text_inputs[\"attention_mask\"],\n",
    "    \"max_structure_len\": 64,  # 中间结构生成最大长度\n",
    "    \"max_seq_len\": 50         # 最终序列生成最大长度\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# 4. 推理（Generate）\n",
    "# -----------------------------\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(batch)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. 解码结果\n",
    "# -----------------------------\n",
    "generated_seq = prostt5_tokenizer.batch_decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. 打印结果\n",
    "# -----------------------------\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated Sequence: {generated_seq[0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text2amp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
